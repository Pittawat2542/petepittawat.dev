[
  {
    "year": 2025,
    "type": "journal",
    "title": "The Role of Large Language Model-Generated Stories in the Narrative Experience of Serious Visual Novel Games",
    "authors": "Mustafa Can Gursesli, Siyuan Chen, Mury Fajar Dewantoro, Xiao You, Ege Anbar, Pittawat Taveekitworachai, Febri Abdullah, Pietro Tarchi, Mirko Duradoni, Antonio Lanata, Andrea Guazzini, and Ruck Thawonmas",
    "venue": "International Journal of Human–Computer Interaction",
    "url": "https://www.tandfonline.com/doi/full/10.1080/10447318.2025.2528993",
    "artifacts": [],
    "tags": [
      "Large Language Models",
      "Prompt Engineering",
      "Procedural Content Generation",
      "Visual Novels",
      "HCI"
    ],
    "abstract": "This study examines the impact of Large Language Model-generated narratives in a climate-change-themed Visual Novel, comparing two versions: First, the story is generated using thematic keywords in the prompts, and second, the story is generated without keywords. Fifty participants (21 female, 29 male) completed the study. Results showed that participants in the group without thematic keywords had higher levels of narrative engageability score, as measured by the Narrative Engageability Scale, than those with thematic keywords. This indicated that the ability to engage with the story was stronger in the group without keywords. However, when assessing the narrative experience using the Game User Experience Satisfaction Scale, both groups reported similar levels of satisfaction, suggesting that while the ability to engage with the narrative differed between groups, the overall narrative experience was mainly the same. These findings suggested that thematic keywords in prompts significantly impacted participants’ narrative experience of the game."
  },
  {
    "year": 2025,
    "type": "journal",
    "title": "BenchING: A Benchmark for Evaluating Large Language Models in Following Structured Output Format Instruction in Text-Based Narrative Game Tasks",
    "authors": "Pittawat Taveekitworachai, Mury F. Dewantoro, Yi Xia, Pratch Suntichaikul, and Ruck Thawonmas",
    "venue": "IEEE Transactions on Games",
    "url": "https://ieeexplore.ieee.org/document/10840256",
    "artifacts": [
      { "label": "Code", "href": "https://github.com/Pittawat2542/llm-output-formats" }
    ],
    "tags": [
      "Large Language Models",
      "Prompt Engineering",
      "Procedural Content Generation",
      "LLM Evaluation"
    ],
    "abstract": "This paper presents BenchING, a new benchmark for evaluating large language models (LLMs) on their ability to follow structured output format instructions in text-based procedural content generation (PCG) tasks. The ability to condition LLMs to output in specified formats proves useful, as downstream components in LLM-integrated games often require structured outputs for exchanging information. However, there is a gap in evaluating this aspect of LLMs, especially in narrative PCG tasks, making it difficult to select LLMs and design games or applications integrating these LLMs. To demonstrate the potential of our benchmark, we evaluate nine LLMs for their ability to generate parseable formatted outputs using five selected text-based PCG tasks. We report on the performance of these LLMs on these tasks. Additionally, we categorize more detailed error types and propose solutions by utilizing LLMs to fix these errors. We also conduct a scaling study, investigating an emergent point of LLMs for their ability to fix malformed formatted content using eight quantized LLMs with varying original sizes from 0.62B to 72.3B. Furthermore, we perform a qualitative study to assess the quality of the generated content. We make our source code and raw data available for future research."
  },
  {
    "year": 2025,
    "type": "preprint",
    "title": "Single Answer is Not Enough: On Generating Ranked Lists with Medical Reasoning Models",
    "authors": "Pittawat Taveekitworachai (co-first author), Natpatchara Pongjirapat (co-first author), Krittaphas Chaisutyakorn, Piyalitt Ittichaiwong, Tossaporn Saengja (co-advisor), and Kunat Pipatanakul (co-advisor)",
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2509.20866",
    "artifacts": [],
    "tags": ["Large Language Models", "Medical AI", "Reasoning Models"],
    "abstract": "This paper presents a systematic study on enabling medical reasoning models (MRMs) to generate ranked lists of answers for open-ended questions. Clinical decision-making rarely relies on a single answer but instead considers multiple options, reducing the risks of narrow perspectives. Yet current MRMs are typically trained to produce only one answer, even in open-ended settings. We propose an alternative format: ranked lists and investigate two approaches: prompting and fine-tuning. While prompting is a cost-effective way to steer an MRM's response, not all MRMs generalize well across different answer formats: choice, short text, and list answers. Based on our prompting findings, we train and evaluate MRMs using supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). SFT teaches a model to imitate annotated responses, and RFT incentivizes exploration through the responses that maximize a reward. We propose new reward functions targeted at ranked-list answer formats, and conduct ablation studies for RFT. Our results show that while some SFT models generalize to certain answer formats, models trained with RFT are more robust across multiple formats. We also present a case study on a modified MedQA with multiple valid answers, finding that although MRMs might fail to select the benchmark's preferred ground truth, they can recognize valid answers. To the best of our knowledge, this is the first systematic investigation of approaches for enabling MRMs to generate answers as ranked lists. We hope this work provides a first step toward developing alternative answer formats that are beneficial beyond single answers in medical domains."
  },
  {
    "year": 2024,
    "type": "journal",
    "title": "Driving Assistant Using Generative AI Pre-generated Messages in Simulator-Based Driving Assessment: A Step Towards Low-Cost Simulator-Based Driving Assessment",
    "authors": "Gunt Chanmas, Pittawat Taveekitworachai, Xiao You, Ruck Thawonmas, Chakarida Nukoolkit, and Piyapat Dajpratham",
    "venue": "Heliyon Vol. 10, Issue 16, E35941",
    "url": "https://www.cell.com/heliyon/fulltext/S2405-8440(24)11972-X",
    "artifacts": [],
    "tags": [
      "Simulator-based Driving Assessment",
      "Post-Stroke Driving",
      "Large Language Models",
      "Vector Database",
      "Efficient LLMs"
    ],
    "abstract": "This paper presents a novel approach for a low-cost simulator-based driving assessment system incorporating a speech-based assistant, using pre-generated messages from Generative AI to achieve real-time interaction during the assessment. Simulator-based assessment is a crucial apparatus in the research toolkit for various fields. Traditional assessment approaches, like on-road evaluation, though reliable, can be risky, costly, and inaccessible. Simulator-based assessment using stationary driving simulators offers a safer evaluation and can be tailored to specific needs. However, these simulators are often only available to research-focused institutions due to their cost. To address this issue, our study proposes a system with the aforementioned properties aiming to enhance drivers' situational awareness, and foster positive emotional states, i.e., high valence and medium arousal, while assessing participants to prevent subpar performers from proceeding to the next stages of assessment and/or rehabilitation. In addition, this study introduces the speech-based assistant which provides timely guidance adaptable to the ever-changing context of the driving environment and vehicle state. The study's preliminary outcomes reveal encouraging progress, highlighting improved driving performance and positive emotional states when participants are engaged with the assistant during the assessment."
  },
  {
    "year": 2024,
    "type": "journal",
    "title": "A Systematic Review of Major Evaluation Metrics for Simulator-Based Automatic Assessment of Driving after Stroke",
    "authors": "Pittawat Taveekitworachai, Gunt Chanmas, Pujana Paliyawan, Ramita Thawonmas, Chakarida Nukoolkit, Piyapat Dajpratham, and Ruck Thawonmas",
    "venue": "Heliyon Vol. 10, Issue 12, E32930",
    "url": "https://www.cell.com/heliyon/fulltext/S2405-8440(24)08961-8",
    "artifacts": [],
    "tags": ["Simulator-based Driving Assessment", "Post-Stroke Driving"],
    "abstract": "Background: Simulator-based driving assessments (SA) have recently been used and studied for various purposes, particularly for post-stroke patients. Automating such assessment has potential benefits especially on reducing financial cost and time. Nevertheless, there currently exists no clear guideline on assessment techniques and metrics available for SA for post-stroke patients. Therefore, this systematic review is conducted to explore such techniques and establish guidelines for evaluation metrics.\nObjective: This review aims to find: (a) major evaluation metrics for automatic SA in post-stroke patients and (b) assessment inputs and techniques for such evaluation metrics.\nMethods: The study follows the PRISMA guideline. Systematic searches were performed on PubMed, Web of Science, ScienceDirect, ACM Digital Library, and IEEE Xplore Digital Library for articles published from January 1, 2010, to December 31, 2023. This review targeted journal articles written in English about automatic performance assessment of simulator-based driving by post-stroke patients. A narrative synthesis was provided for the included studies.\nResults: The review included six articles with a total of 239 participants. Across all of the included studies, we discovered 49 distinct assessment inputs. Threshold-based, machine-learning-based, and driving simulator calculation approaches are three primary types of assessment techniques and evaluation metrics identified in the review.\nDiscussion: Most studies incorporated more than one type of input, indicating the importance of a comprehensive evaluation of driving abilities. Threshold-based techniques and metrics were the most commonly used in all studies, likely due to their simplicity. An existing relevant review also highlighted the limited number of studies in this area, underscoring the need for further research to establish the validity and effectiveness of simulator-based automatic assessment of driving (SAAD).\nConclusions: More studies should be conducted on various aspects of SAAD to explore and validate this type of assessment."
  },
  {
    "year": 2024,
    "type": "journal",
    "title": "The 1st ChatGPT4PCG Competition",
    "authors": "Febri Abdullah, Pittawat Taveekitworachai, Mury F. Dewantoro, Ruck Thawonmas, Julian Togelius, and Jochen Renz",
    "venue": "IEEE Transactions on Games",
    "url": "https://ieeexplore.ieee.org/abstract/document/10470422",
    "artifacts": [],
    "tags": [
      "Large Language Models",
      "Prompt Engineering",
      "Procedural Content Generation",
      "Level Generation",
      "LLM Evaluation"
    ],
    "abstract": "This article summarizes the first ChatGPT4PCG competition held at the 2023 IEEE Conference on Games. The goal of the competition is to explore emergent abilities of publicly available large language models (LLMs) in performing complex tasks related to procedural content generation, specifically physics-based level generation for Angry Birds-like games. Participants are tasked with submitting their prompts for ChatGPT to generate Angry Birds-like game structures that resemble English uppercase characters. A structure is a collection of stacked game objects comprising a part of an entire Angry Birds-like level. A prompt is an input for LLMs, including ChatGPT. Two evaluation metrics, i.e., stability and similarity, are used to evaluate the submitted prompts. Stability measures the sturdiness of a structure to withstand in-game gravity, while similarity measures a structure's resemblance to the target character. With such evaluation, participants are challenged to produce not only character-like but also stable structures by utilizing prompt engineering techniques. Finally, the competition's results are discussed to provide valuable insights for future studies and competitions."
  },
  {
    "year": 2025,
    "type": "conference",
    "title": "Prior Prompt Engineering for Reinforcement Fine-Tuning",
    "authors": "Pittawat Taveekitworachai, Potsawee Manakul, Sarana Nutanong, and Kunat Pipatanakul",
    "venue": "The Conference on Empirical Methods in Natural Language Processing (EMNLP) 2025, Main",
    "url": "https://arxiv.org/abs/2505.14157",
    "artifacts": [],
    "tags": [
      "Large Language Models",
      "Prompt Engineering",
      "Reinforcement Learning",
      "Reinforcement Fine-Tuning",
      "Reasoning Models",
      "Prior Prompts"
    ],
    "abstract": "This paper investigates prior prompt engineering (pPE) in the context of reinforcement fine-tuning (RFT), where language models (LMs) are incentivized to exhibit behaviors that maximize performance through reward signals. While existing RFT research has primarily focused on algorithms, reward shaping, and data curation, the design of the prior prompt--the instructions prepended to queries during training to elicit behaviors such as step-by-step reasoning--remains underexplored. We investigate whether different pPE approaches can guide LMs to internalize distinct behaviors after RFT. Inspired by inference-time prompt engineering (iPE), we translate five representative iPE strategies--reasoning, planning, code-based reasoning, knowledge recall, and null-example utilization--into corresponding pPE approaches. We experiment with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and GPQA-Diamond). Our results show that all pPE-trained models surpass their iPE-prompted counterparts, with the null-example pPE approach achieving the largest average performance gain and the highest improvement on AIME2024 and GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by adapting a behavior-classification framework, we demonstrate that different pPE strategies instill distinct behavioral styles in the resulting models. These findings position pPE as a powerful yet understudied axis for RFT."
  },
  {
    "year": 2025,
    "type": "conference",
    "title": "Typhoon T1: An Open Thai Reasoning Model",
    "authors": "Pittawat Taveekitworachai, Potsawee Manakul, Kasima Tharnpipitchai, and Kunat Pipatanakul",
    "venue": "International Conference on Learning Representations (ICLR) - Open Science for Foundation Models (SCI-FM) 2025 Workshop",
    "url": "https://arxiv.org/abs/2502.09042",
    "artifacts": [
      {
        "label": "Typhoon T1 (Research Preview)",
        "href": "https://huggingface.co/scb10x/llama3.2-typhoon2-t1-3b-research-preview"
      }
    ],
    "tags": ["Large Language Models", "Reasoning Models", "Supervised Fine-Tuning"],
    "abstract": "This paper introduces Typhoon T1, an open effort to develop an open Thai reasoning model. A reasoning model is a relatively new type of generative model built on top of large language models (LLMs). A reasoning model generates a long chain of thought before arriving at a final answer, an approach found to improve performance on complex tasks. However, details on developing such a model are limited, especially for reasoning models that can generate traces in a low-resource language. Typhoon T1 presents an open effort that dives into the details of developing a reasoning model in a more cost-effective way by leveraging supervised fine-tuning using open datasets, instead of reinforcement learning. This paper shares the details about synthetic data generation and training, as well as our dataset and model weights. Additionally, we provide insights gained from developing a reasoning model that generalizes across domains and is capable of generating reasoning traces in a low-resource language, using Thai as an example. We hope this open effort provides a foundation for further research in this field."
  },
  {
    "year": 2025,
    "type": "conference",
    "title": "Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging -- An Open Recipe",
    "authors": "Kunat Pipatanakul, Pittawat Taveekitworachai, Potsawee Manakul, and Kasima Tharnpipitchai",
    "venue": "International Conference on Learning Representations (ICLR) - Open Science for Foundation Models (SCI-FM) 2025 Workshop",
    "url": "https://arxiv.org/abs/2502.09056",
    "artifacts": [
      {
        "label": "Typhoon R1 (Research Preview)",
        "href": "https://huggingface.co/scb10x/llama3.1-typhoon2-deepseek-r1-70b-preview"
      }
    ],
    "tags": ["Large Language Models", "Reasoning Models", "Model Merging"],
    "abstract": "This paper investigates data selection and model merging methodologies aimed at incorporating advanced reasoning capabilities such as those of DeepSeek R1 into language-specific large language models (LLMs), with a particular focus on the Thai LLM. Our goal is to enhance the reasoning capabilities of language-specific LLMs while maintaining their target language abilities. DeepSeek R1 excels in reasoning but primarily benefits high-resource languages such as English and Chinese. However, low-resource languages remain underserved due to the dominance of English-centric training data and model optimizations, which limit performance in these languages. This limitation results in unreliable code-switching and diminished effectiveness on tasks in low-resource languages. Meanwhile, local and regional LLM initiatives have attempted to bridge this gap by developing language-specific LLMs that focus on improving local linguistic fidelity. We demonstrate that, with only publicly available datasets and a computational budget of $120, it is possible to enhance the reasoning capabilities of language-specific LLMs to match the level of DeepSeek R1, without compromising their performance on target language tasks."
  },
  {
    "year": 2024,
    "type": "conference",
    "title": "Temporal Collage Prompting: A Cost-Effective Simulator-Based Driving Accident Video Recognition With GPT-4o",
    "authors": "Pratch Suntichaikul, Pittawat Taveekitworachai, Chakarida Nukoolkit, and Ruck Thawonmas",
    "venue": "8th International Conference on Information Technology (Best Paper Award)",
    "url": "https://ieeexplore.ieee.org/document/10810536",
    "artifacts": [],
    "tags": [
      "Temporal Collage Prompting",
      "Large Multimodal Models",
      "Simulator-Based Driving Accident Video Recognition"
    ],
    "abstract": "This paper presents temporal collage prompting, a novel approach for detecting and classifying simulator-based driving accident videos using GPT-4o. While recognizing accident videos is crucial for assessing drivers’ abilities and safety, this task traditionally relies on human labor. In addition, it is time-consuming and inefficient, especially when dealing with numerous videos. Large multi-modal models (LMMs) offer a promising solution to reduce processing time but face a challenge with context window limitation when handling video data. We address this by developing a method that optimizes input efficiency while preserving temporal information. Our approach combines multiple video frames into a collage, significantly reducing input tokens. Testing with custom scenarios generated from CARLA, a driving simulator, we achieve 93% accuracy in accident recognition using a 2x2 collage at 1 frame per second (FPS), outperforming a uniform frames baseline method. This configuration reduces token usage by 91% compared to uniform frames at 3 FPS, while improving accuracy from 72% to 93%. Through ablation studies with various collage layouts and sampling rates, we found that lower frame rates, particularly 1 FPS, are more effective for this task. Our results demonstrate that optimized frame sampling and collage creation can enhance both efficiency and accuracy of video recognition using LMMs, offering a promising solution for simulator-based driving video recognition in driving assessment, with potential applications in other domains requiring temporal visual recognition. We provide our data and source code for public use."
  },
  {
    "year": 2024,
    "type": "conference",
    "title": "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination",
    "authors": "Pittawat Taveekitworachai, Febri Abdullah, and Ruck Thawonmas",
    "venue": "The Conference on Empirical Methods in Natural Language Processing (EMNLP) 2024, Main",
    "url": "https://aclanthology.org/2024.emnlp-main.740/",
    "artifacts": [
      { "label": "Code", "href": "https://github.com/Pittawat2542/null-shot-prompting" },
      { "label": "Data", "href": "https://github.com/Pittawat2542/null-shot-results" }
    ],
    "tags": ["Null-Shot Prompting", "Hallucination", "Large Language Models", "LLM Evaluation"],
    "abstract": "This paper presents a series of investigations into an interesting phenomenon where we observe performance increases in large language models (LLMs) when providing a prompt that causes and exploits hallucination. We propose null-shot prompting, a counter-intuitive approach where we intentionally instruct LLMs to look at and utilize information from a null section. We investigate null-shot prompting on a wide range of tasks, including arithmetic reasoning, commonsense reasoning, and reading comprehension. We observe a substantial increase in performance in arithmetic reasoning tasks for various models, with up to a 44.62% increase compared to a baseline in one model. Therefore, we investigate deeper into this task by utilizing a more challenging mathematics problem-solving benchmark. We observe that LLMs benefit from hallucination in null-shot prompting in this task and discuss the mathematical topics that benefit the most from introducing hallucination in the prompt. We continue our investigation by evaluating hallucination detection abilities of the LLMs when using null-shot prompting. We find surprising results where hallucination in prompts can improve hallucination detection abilities of many LLMs. We also examine the effects of introducing both reasoning, which is known to mitigate hallucination, and hallucination simultaneously in the prompt and observe another surprising turn for the mathematics problem-solving benchmark with many performance improvements. We hope this paper will spark more interest, investigations, and discussions on how hallucination in prompts LLMs and even bolsters them in certain cases."
  },
  {
    "year": 2024,
    "type": "conference",
    "title": "Don't Do That! Reverse Role Prompting Helps Large Language Models Stay in Personality Traits",
    "authors": "Siyuan Chen, Pittawat Taveekitworachai, Yi Xia, Xiaoxu Li, Mustafa Can Gursesli, Antonio Lanata, Andrea Guazzini, and Ruck Thawonmas",
    "venue": "International Conference on Interactive Digital Storytelling (ICIDS) 2024",
    "url": "https://link.springer.com/chapter/10.1007/978-3-031-78453-8_7",
    "artifacts": [],
    "tags": [
      "Role Prompting",
      "Reverse Role Prompting",
      "Large Language Models",
      "Game Story Generation",
      "Biases",
      "LLM Evaluation"
    ],
    "abstract": "This paper investigates effectiveness of role prompting, an approach used to condition a large language model (LLM) with a role or personality trait. Conditioning an LLM with a specific role or personality trait is crucial for various applications, such as using an LLM as a non-playable character. Role-playing is one of the crucial techniques in doing so. However, existing studies have only observed changes when introducing a role or personality trait to LLMs, but not how effectively they conform to the given personality trait. In this study, we investigate how well LLMs adhere to given instructions using a well-established personality test. Additionally, we conduct an experiment to determine how a given personality influences biases in generating a game story ending. Through our investigations, we found that traditional role prompting is ineffective for assessing a certain personality trait. Therefore, we propose a novel variant of role prompting called reverse role prompting (RRP) to reduce the significance of personality traits other than the assigned personality. We observed that when using RRP to assign personalities to LLMs, the LLMs are better able to act the given personality. We also found that LLMs inherently have high agreeableness, affecting story ending biases when instructed to generate a game story. RRP is also effective for reducing the effects of this inherent agreeableness. Future studies should further investigate the effectiveness of the proposed RRP for a wider range of narrative elements and applications in storytelling."
  },
  {
    "year": 2024,
    "type": "conference",
    "title": "LLMs in Eduverse: LLM-Integrated English Educational Game in Metaverse",
    "authors": "Kantinan Plupattanakit, Pratch Suntichaikul, Pittawat Taveekitworachai, Ruck Thawonmas, Jeremy White, Kingkarn Sookhanaphibarn, and Worawat Choensawat",
    "venue": "IEEE Global Conference on Consumer Electronics (GCCE) 2024",
    "url": "https://ieeexplore.ieee.org/document/10760474",
    "artifacts": [],
    "tags": [
      "Large Language Models",
      "Prompt Engineering",
      "Metaverse",
      "Serious Games (Education)"
    ],
    "abstract": "This demo paper presents a new educational game on the metaverse platform Roblox, integrating large language models (LLMs) for dynamic content generation. The game is suitable for up to five players and includes a single-player mode. It is based on the word-guessing genre where players guess a target word from a list of ten options using hints and feedback on the semantic closeness of their guesses. Utilizing GPT-3.5 Turbo, we generate semantic similarity scores and hints, proposing a new prompt to instruct LLMs for this purpose. To increase student engagement, our game leverages Roblox’s scoring and spotlight systems. We address stochastic behavior and unreliable output in LLM integration by presenting a series of prompts and a useful script. Our contributions include suggestions for LLM integration in educational games and a prototype demonstrating these concepts."
  },
  {
    "year": 2024,
    "type": "conference",
    "title": "Assessing Inherent Biases Following Prompt Compression of Large Language Models for Game Story Generation",
    "authors": "Pittawat Taveekitworachai, Kantinan Plupattanakit, and Ruck Thawonmas",
    "venue": "IEEE Conference on Games (CoG) 2024",
    "url": "https://ieeexplore.ieee.org/document/10645609",
    "artifacts": [
      {
        "label": "Research artifacts",
        "href": "https://github.com/Pittawat2542/prompt-compression-biases"
      }
    ],
    "tags": [
      "Large Language Models",
      "Prompt Engineering",
      "Prompt Compression",
      "Biases",
      "Procedural Content Generation",
      "Story Generation",
      "LLM Evaluation"
    ],
    "abstract": "This paper investigates how prompt compression, a technique to reduce the number of tokens in the prompt while maintaining prompt performance, affects inherent biases in large language models (LLMs) for the story ending of the game story generation task. Previous studies have explored inherent biases in LLMs and found an innate inclination of LLMs towards generating positive-ending stories. While prompt compression is known to retain task performance and utilize fewer tokens in the prompt, we explore a different perspective on how prompt compression could affect inherent biases in LLMs. We follow existing studies’ approach in evaluating story ending biases of six LLMs comparing uncompressed and compressed prompts. We find that prompt compression does not affect story generation from positive-ending story synopses, to which these LLMs are inclined. However, it is not the same for negative-ending story synopses: prompt compression either makes the LLMs generate a higher amount of negative-ending stories or not at all. We also notice that the classification of other types of story endings, other than those specified in the prompt, aligns with an existing study. We recommend game developers and future studies to always perform empirical tests on prompt compression, as it is not straightforward and may greatly alter model behaviors."
  },
  {
    "year": 2024,
    "type": "conference",
    "title": "Towards LLM4PCG: A Preliminary Evaluation of Open-Weight Large Language Models Beyond ChatGPT4PCG",
    "authors": "Pittawat Taveekitworachai, Yi Xia, Pratch Suntichaikul, and Ruck Thawonmas",
    "venue": "IEEE Conference on Games (CoG) 2024",
    "url": "https://ieeexplore.ieee.org/document/10645646",
    "artifacts": [
      { "label": "Research artifacts 1", "href": "https://github.com/Pittawat2542/llm4pcg-python" },
      {
        "label": "Research artifacts 2",
        "href": "https://github.com/Pittawat2542/llm4pcg-experiment"
      }
    ],
    "tags": [
      "Large Language Models",
      "Prompt Engineering",
      "Procedural Content Generation",
      "Level Generation",
      "LLM Evaluation"
    ],
    "abstract": "This paper presents an initial step towards general evaluations of open-weight large language models (LLMs) using the ChatGPT4PCG platform, a Science Birds level generation challenge designed to evaluate LLMs on the complex task of generating stable, English-character-resembling, and diverse levels. While ChatGPT4PCG competitions have their own merit in providing a comprehensive platform for evaluating ChatGPT on complex tasks, the competitions focus solely on ChatGPT is rather limiting considering the fact that there are many available choices of open-weight LLMs. We report 13 LLMs from five model families of various properties in their design choices and sizes to evaluate on a modified ChatGPT4PCG 2 competition platform. We observe that the scaling law holds in general, but the inherent capabilities of LLMs due to their pre-training and architecture choices also play an equal role. We open-source the modification of the ChatGPT4PCG platform to support future research on evaluating LLMs in this area"
  },
  {
    "year": 2024,
    "type": "conference",
    "title": "ChatGPT4PCG 2 Competition: Prompt Engineering for Science Birds Level Generation",
    "authors": "Pittawat Taveekitworachai, Febri Abdullah, Mury F. Dewantoro, Yi Xia, Pratch Suntichaikul, Ruck Thawonmas, Julian Togelius, and Jochen Renz",
    "venue": "IEEE Conference on Games (CoG) 2024",
    "url": "https://ieeexplore.ieee.org/document/10645641",
    "artifacts": [
      { "label": "Research artifacts", "href": "https://github.com/orgs/chatgpt4pcg/repositories" }
    ],
    "tags": [
      "Large Language Models",
      "Prompt Engineering",
      "Procedural Content Generation",
      "Level Generation",
      "LLM Evaluation"
    ],
    "abstract": "This paper presents the second ChatGPT4PCG competition at the 2024 IEEE Conference on Games. In this edition of the competition, we follow the first edition, but make several improvements and changes. We introduce a new evaluation metric along with allowing a more flexible format for participants’ submissions and making several improvements to the evaluation pipeline. Continuing from the first edition, we aim to foster and explore the realm of prompt engineering (PE) for procedural content generation (PCG). While the first competition saw success, it was hindered by various limitations; we aim to mitigate these limitations in this edition. We introduce diversity as a new metric to discourage submissions aimed at producing repetitive structures. Furthermore, we allow submission of a Python program instead of a prompt text file for greater flexibility in implementing advanced PE approaches, which may require control flow, including conditions and iterations. We also make several improvements to the evaluation pipeline with a better classifier for similarity evaluation and better-performing function signatures. We thoroughly evaluate the effectiveness of the new metric and the improved classifier. Additionally, we perform an ablation study to select a function signature to instruct ChatGPT for level generation. Finally, we provide implementation examples of various PE techniques in Python and evaluate their preliminary performance. We hope this competition serves as a resource and platform for learning about PE and PCG in general"
  },
  {
    "year": 2024,
    "type": "conference",
    "title": "Dungeons, Dragons, and Emotions: A Preliminary Study of Player Sentiment in LLM-driven TTRPGs",
    "authors": "Xiao You, Pittawat Taveekitworachai, Mustafa Can Gursesli, Xiaoxu Li, Yi Xia and Ruck Thawonmas",
    "venue": "ACM International Conference on the Foundations of Digital Games (FDG) 2024",
    "url": "https://dl.acm.org/doi/10.1145/3649921.3656991",
    "artifacts": [],
    "tags": [
      "Large Language Models",
      "LLM-Powered Games",
      "Tabletop RPGs",
      "Prompt Engineering",
      "Peronsality Prompting"
    ],
    "abstract": "In this paper, we present a Tabletop Role-Playing game (TTRPG) driven by ChatGPT. Prompts are employed to instruct ChatGPT to act as Game Masters (GMs). In crafting each prompt to integrate a distinctive role, three roles denoted as Role 1, Role 2, and Role 3, are established. Subsequently, we perform pre-game and post-game emotional assessments employing the Positive and Negative Affect Schedule (PANAS) questionnaire to scrutinize players’ emotional dynamics throughout the gaming experience. Upon analyzing the collected data, we observe that Role 1 and Role 2 affect players’ positive emotions. Notably, Role 2 exhibits the most pronounced influence on players’ positive emotions. Our findings demonstrate that a TTRPG GM powered by ChatGPT can significantly enhance players’ positive emotions. This leads us to recognize that TTRPG GM powered by ChatGPT plays a positive role in enhancing the mental well-being of specific populations."
  },
  {
    "year": 2024,
    "type": "conference",
    "title": "Speed Up! Cost-Effective Large Language Model for ADAS Via Knowledge Distillation",
    "authors": "Pittawat Taveekitworachai, Pratch Suntichaikul, Chakarida Nukoolkit, and Ruck Thawonmmas",
    "venue": "IEEE Intelligent Vehicles Symposium (IV 2024) (Nominated for Best Paper)",
    "url": "https://ieeexplore.ieee.org/document/10588799",
    "artifacts": [
      {
        "label": "Research artifacts",
        "href": "https://github.com/Pittawat2542/driving-assessment-distillation"
      }
    ],
    "tags": [
      "Large Language Models",
      "Knowledge Distillation",
      "Efficient LLMs",
      "Prompt Engineering",
      "Vector Database",
      "Advanced Driver-Assistance System",
      "LLM Evaluation"
    ],
    "abstract": "This paper presents a cost-effective approach to utilizing large language models (LLMs) as part of advanced driver-assistance systems (ADAS) through a knowledge-distilled model for driving assessment. LLMs have recently been employed across various domains. However, due to their size, they require sufficient computing infrastructure for deployment and ample time for generation. These characteristics make LLMs challenging to integrate into applications requiring real-time feedback, including ADAS. An existing study employed a vector database containing responses generated from an LLM to act as a surrogate model. However, this approach is limited when handling out-of-distribution (OOD) scenarios, which LLMs excel at. We propose a novel approach that utilizes a distilled model obtained from an established knowledge distillation technique to perform as a surrogate model for a target LLM, offering high resilience in handling OOD situations with substantially faster inference time. To assess the performance of the proposed approach, we also introduce a new dataset for driving scenarios and situations (DriveSSD), containing 124,248 records. Additionally, we augment randomly selected 12,425 records, 10% of our DriveSSD, with text embeddings generated from an embedding model. We distill the model using 10,000 augmented records and test all approaches on the remaining 2,425 records. We find that the distilled model introduced in this study has better performance across metrics, with half of the inference time used by the previous approach. We make our source code and data publicly available."
  },
  {
    "year": 2024,
    "type": "conference",
    "title": "Prompt Evolution Through Examples for Large Language Models—A Case Study in Game Comment Toxicity Classification",
    "authors": "Pittawat Taveekitworachai, Febri Abdullah, Mustafa Can Gursesli, Antonio Lanata, Andrea Guazzini, and Ruck Thawonmmas",
    "venue": "IEEE International Workshop on Metrology for Industry 4.0 & IoT 2024",
    "url": "https://ieeexplore.ieee.org/document/10584130",
    "artifacts": [
      {
        "label": "Research artifacts",
        "href": "https://github.com/Pittawat2542/pete-prompt-optimization"
      }
    ],
    "tags": [
      "Large Language Models",
      "Prompt Engineering",
      "Automatic Prompt Optimization",
      "Errorful Learning",
      "Evolutionary Computation",
      "Toxicity Classification",
      "LLM Evaluation"
    ],
    "abstract": "This paper presents a novel approach for automatic prompt optimization (APO) using a large language model (LLM) as an optimizer, named Prompt Evolution Through Examples (PETE). The approach draws inspiration from evolutionary computation for the prompt evolution stages. We aim to aid in developing prompts for use in systems classifying toxic content including game community moderator-assist tools. While traditional approaches are useful for developing these tools, they have various shortcomings where LLMs can potentially mitigates these issues. LLMs accept prompts as inputs to condition generated outputs. However, to design a prompt with the best performance in this task, fine-grained adjustments are usually required and should be automated through the APO process instead of a manual approach, which is often time-consuming. In this study, ChatGPT and GPT-4 are utilized as both task performers and prompt optimizers for comparisons across models. The results indicate that PETE improves the performance of the target task up to 56.14% from a performance of an initial prompt, compared to only up to 49.15% using a standard mutation evolution. Optimized prompts are provided for future utilization in other game community moderation tools. We also recommend that future studies explore more cost-effective approaches for evaluation using LLMs to enhance the benefits of APO."
  },
  {
    "year": 2023,
    "type": "conference",
    "title": "Enhancing Novelty in ChatGPT Responses: Incorporating Random Word Brainstorming",
    "authors": "Pittawat Taveekitworachai and Ruck Thawonmas",
    "venue": "13th International Conference on Advances in Information Technology (IAIT)",
    "url": "https://dl.acm.org/doi/abs/10.1145/3628454.3628456",
    "artifacts": [
      {
        "label": "Research artifacts",
        "href": "https://github.com/Pittawat2542/chatgpt-idea-random-words"
      }
    ],
    "tags": [
      "Large Language Models",
      "Prompt Engineering",
      "Creavitity in LLMs",
      "Random Word Brainstorming"
    ],
    "abstract": "This paper presents a new prompting approach for increasing the novelty in ChatGPT responses. ChatGPT has proven to be effective in generating natural language responses; however, ensuring response novelty remains challenging. Our proposed method, inspired by random word brainstorming, includes random words in prompts to introduce more diversity in ChatGPT responses. Through a questionnaire-based evaluation, we compared preferences for solution ideas generated using the standard approach and our proposed approach. We found that participants preferred our technique in 65% of the 20 problems. The results suggest the effectiveness of our proposed approach. We also explored the use of GPT models as evaluators, with GPT-3.5 achieving 65% accuracy and GPT-4 achieving 70% accuracy when compared to human preferences from the questionnaire. These results suggest the potential of leveraging GPT models as noisy natural language evaluators. For future studies, we recommend focusing on prompt engineering and word list design to further improve performance. Overall, incorporating random words in prompts can effectively increase novelty in ChatGPT responses."
  },
  {
    "year": 2023,
    "type": "conference",
    "title": "Am I Fighting Well? Fighting Game Commentary Generation With ChatGPT",
    "authors": "Chollakorn Nimpattanavong, Pittawat Taveekitworachai, Ibrahim Khan, Thai Van Nguyen, Ruck Thawonmas, Worawat Choensawat, and Kingkarn Sookhanaphibarn",
    "venue": "13th International Conference on Advances in Information Technology (IAIT)",
    "url": "https://dl.acm.org/doi/abs/10.1145/3628454.3629551",
    "artifacts": [
      {
        "label": "Research artifacts",
        "href": "https://github.com/Pittawat2542/chatgpt-game-comment"
      }
    ],
    "tags": [
      "Large Language Models",
      "LLM Evaluation",
      "Prompt Engineering",
      "Game Commentary Generation"
    ],
    "abstract": "This paper presents a new approach for leveraging ChatGPT in fighting game commentary generation task. Commentary generation often relies on deep learning techniques, which typically demand extensive data to achieve effectiveness. Large language models (LLMs) have become essential due to their remarkable ability to process data efficiently, thanks to their extensive training on vast datasets. Our proposed approach integrates the use of LLMs, specifically the GPT-3.5 model, for generating commentaries through the utilization of various prompts with data from the open-source fighting game, DareFightingICE. Four prompt variants are employed to assess the effectiveness of each prompt components. Objective evaluation using natural language metrics reveals that different prompt components significantly affect the generated commentaries. Additionally, subjective evaluation through a questionnaire reveals that prompts without parameter definitions received the highest preference from human evaluators. These results suggest that LLMs exhibit versatility in generating fighting game commentaries and hold promise for broader applications."
  },
  {
    "year": 2023,
    "type": "conference",
    "title": "ChatGPT4PCG Competition: Character-like Level Generation for Science Birds",
    "authors": "Pittawat Taveekitworachai, Febri Abdullah, Mury F. Dewantoro, Ruck Thawonmas, Julian Togelius, and Jochen Renz",
    "venue": "IEEE Conference on Games (CoG) 2023",
    "url": "https://ieeexplore.ieee.org/abstract/document/10333206",
    "artifacts": [
      { "label": "Research artifacts", "href": "https://github.com/orgs/chatgpt4pcg/repositories" }
    ],
    "tags": [
      "Large Language Models",
      "Prompt Engineering",
      "Procedural Content Generation",
      "Level Generation",
      "LLM Evaluation"
    ],
    "abstract": "This paper presents the first ChatGPT4PCG Competition at the 2023 IEEE Conference on Games. The objective of this competition is for participants to create effective prompts for ChatGPT–enabling it to generate Science Birds levels with high stability and character-like qualities–fully using their creativity as well as prompt engineering skills. ChatGPT is a conversational agent developed by OpenAI. Science Birds is selected as the competition platform because designing an Angry Birds-like level is not a trivial task due to the in-game gravity; the quality of the levels is determined by their stability. To lower the entry barrier to the competition, we limit the task to the generation of capitalized English alphabetical characters. We also allow only a single prompt to be used for generating all the characters. Here, the quality of the generated levels is determined by their stability and similarity to the given characters. A sample prompt is provided to participants for their reference. An experiment is conducted to determine the effectiveness of several modified versions of this sample prompt on level stability and similarity by testing them on several characters. To the best of our knowledge, we believe that ChatGPT4PCG is the first competition of its kind and hope to inspire enthusiasm for prompt engineering in procedural content generation."
  },
  {
    "year": 2023,
    "type": "conference",
    "title": "What Is Waiting for Us at the End? Inherent Biases of Game Story Endings in Large Language Models",
    "authors": "Pittawat Taveekitworachai, Febri Abdullah, Mustafa Can Gursesli, Mury F. Dewantoro, Siyuan Chen, Antonio Lanata, Andrea Guazzini, and Ruck Thawonmmas",
    "venue": "International Conference on Interactive Digital Storytelling (ICIDS) 2023",
    "url": "https://link.springer.com/chapter/10.1007/978-3-031-47658-7_26",
    "artifacts": [],
    "tags": [
      "Large Language Models",
      "Biases in LLMs",
      "Procedural Content Generation",
      "Story Generation",
      "LLM Evaluation",
      "Sentiment Analysis"
    ],
    "abstract": "This study investigates biases present in large language models (LLMs) when utilized for narrative tasks, specifically in game story generation and story ending classification. Our experiment involves using popular LLMs, including GPT-3.5, GPT-4, and Llama 2, to generate game stories and classify their endings into three categories: positive, negative, and neutral. The results of our analysis reveal a notable bias towards positive-ending stories in the LLMs under examination. Moreover, we observe that GPT-4 and Llama 2 tend to classify stories into uninstructed categories, underscoring the critical importance of thoughtfully designing downstream systems that employ LLM-generated outputs. These findings provide a groundwork for the development of systems that incorporate LLMs in game story generation and classification. They also emphasize the necessity of being vigilant in addressing biases and improving system performance. By acknowledging and rectifying these biases, we can create more fair and accurate applications of LLMs in various narrative-based tasks."
  },
  {
    "year": 2023,
    "type": "conference",
    "title": "Breaking bad: Unraveling Influences and Risks of User Inputs to ChatGPT for Game Story Generation",
    "authors": "Pittawat Taveekitworachai, Febri Abdullah, Mury F. Dewantoro, Yi Xia, Pratch Suntichaikul, Ruck Thawonmmas, Julian Togelius, and Jochen Renz",
    "venue": "International Conference on Interactive Digital Storytelling (ICIDS) 2023",
    "url": "https://link.springer.com/chapter/10.1007/978-3-031-47658-7_27",
    "artifacts": [
      {
        "label": "Research artifacts",
        "href": "https://github.com/Pittawat2542/chatgpt-words-influence-risks"
      }
    ],
    "tags": [
      "Large Language Models",
      "Risks in LLMs",
      "Procedural Content Generation",
      "Story Generation",
      "Adversarial Prompting",
      "LLM Evaluation",
      "Sentiment Analysis"
    ],
    "abstract": "This study presents an investigation into the influence and potential risks of using user inputs as part of a prompt, a message used to interact with ChatGPT. We demonstrate the influence of user inputs in a prompt through game story generation and story ending classification. To assess risks, we utilize a technique called adversarial prompting, which involves deliberately manipulating the prompt or parts of the prompt to exploit the safety mechanisms of large language models, leading to undesirable or harmful responses. We assess the influence of positive and negative sentiment words, as proxies for user inputs in a prompt, on the generated story endings. The results suggest that ChatGPT tends to adhere to its guidelines, providing safe and non-harmful outcomes, i.e., positive endings. However, malicious intentions, such as “jailbreaking”, can be achieved through prompting injection. These actions carry significant risks of producing unethical outcomes, as shown in an example. As a result, this study also suggests preliminary ways to mitigate these risks: content filtering, rare token-separators, and enhancing training datasets and alignment processes."
  },
  {
    "year": 2023,
    "type": "conference",
    "title": "The Chronicles of ChatGPT: Generating and Evaluating Visual Novel Narratives on Climate Change Through ChatGPT",
    "authors": "Mustafa Can Gursesli, Pittawat Taveekitworachai, Febri Abdullah, Mury F. Dewantoro, Antonio Lanata, Andrea Guazzini, and Ruck Thawonmmas",
    "venue": "International Conference on Interactive Digital Storytelling (ICIDS) 2023",
    "url": "https://link.springer.com/chapter/10.1007/978-3-031-47658-7_16",
    "artifacts": [
      {
        "label": "Research artifacts",
        "href": "https://github.com/Pittawat2542/chatgpt-visual-novel-evaluation"
      }
    ],
    "tags": [
      "Large Language Models",
      "Prompt Engineering",
      "Procedural Content Generation",
      "Story Generation",
      "LLM Evaluation",
      "Visual Novels"
    ],
    "abstract": "This paper explores the potential of utilizing ChatGPT, a large language model (LLM), for generating and evaluating visual novel (VN) game stories in the context of global warming awareness through a VN game. The study involves generating two stories using ChatGPT, one with given global warming related keywords as an inspiration for ChatGPT along with a specified ending and another without, and evaluating them based on several linguistic criteria: coherence, inspiration, readability, word complexity, and narrative fluency. Results reveal that keywords-inspired story exhibit higher coherence, while the basic one demonstrate greater inspiration. The findings highlight the advantages of each story and emphasize the value of AI-driven narrative generation in creating engaging and informative experiences. Furthermore, the study introduces an innovative approach by employing ChatGPT as an evaluator for the story quality, by combining various prompt engineering techniques showcasing the diverse applications of LLMs in interactive storytelling. This work contributes to the growing field of LLM-based story generation and underscores the potential of AI-driven narratives in fostering awareness and engagement on critical issues like climate change."
  },
  {
    "year": 2023,
    "type": "conference",
    "title": "Analyzing Audience Comments: Improving Interactive Narrative with ChatGPT",
    "authors": "Xiaoxu Li, Xiao You, Siyuan Chen, Pittawat Taveekitworachai, and Ruck Thawonmmas",
    "venue": "International Conference on Interactive Digital Storytelling (ICIDS) 2023",
    "url": "https://link.springer.com/chapter/10.1007/978-3-031-47658-7_20",
    "artifacts": [],
    "tags": [
      "Large Language Models",
      "Audience Participation Games",
      "Text Classification",
      "LLM Evaluation"
    ],
    "abstract": "This paper presents a novel method that utilizes ChatGPT for the categorization of audience comments in game live streams, treating it as a zero-shot task. Audience participation games have gained significant popularity in the realm of game live streaming, playing a vital role in game promotion and audience engagement. Streamers employ various techniques such as storytelling and interactive narrative to cultivate a larger fan base and enhance the value of their streams. Simultaneously, the audience generates diverse comments that directly impact the streamer’s interactive narrative and storytelling. However, the traditional methods for comment analysis in game live streams are lacking in terms of speed and cost-effectiveness. Therefore, our aim is to investigate whether ChatGPT can fulfill these requirements. Through experimental evaluation, our results indicate a majority choice of 54.34% and a human choice of 82.61%, showcasing that ChatGPT, when employed with suitable prompts, can address the aforementioned need."
  },
  {
    "year": 2023,
    "type": "conference",
    "title": "Journey of ChatGPT from Prompts to Stories in Games: the Positive, the Negative, and the Neutral",
    "authors": "Pittawat Taveekitworachai, Mustafa Can Gursesli, Febri Abdullah, Siyuan Chen, Federico Cala, Andrea Guazzini, Antonio Lanata, and Ruck Thawonmmas",
    "venue": "IEEE International Conference on Consumer Electronic (ICCE)-Berlin 2023",
    "url": "https://ieeexplore.ieee.org/abstract/document/10375663",
    "artifacts": [
      {
        "label": "Research artifacts",
        "href": "https://github.com/Pittawat2542/chatgpt-game-story-generation"
      }
    ],
    "tags": [
      "Large Language Models",
      "Biases in LLMs",
      "Procedural Content Generation",
      "Story Generation",
      "LLM Evaluation",
      "Sentiment Analysis"
    ],
    "abstract": "This paper presents a study on biases present in the story endings for story-driven games generated by ChatGPT. The study uses various prompts to assess the biases in ChatGPT's output. The results emphasize a consistent inclination towards positive endings in the stories generated by ChatGPT. Even when explicitly instructed to generate neutral endings, ChatGPT exhibited a bias towards positive outcomes. These biases raise concerns regarding the training data and alignment processes used by OpenAI to train ChatGPT, as they may reflect societal biases or the preferences of the majority of the data. Addressing these biases is crucial to ensure that these models align with societal norms and avoid reinforcing existing biases. Future studies should concentrate on developing methods to reduce biases in AI language models and enhance the ethical perspective of these technologies. Our source code and data are made publicly available at https://bit.ly/chatgpt-game-story-gen."
  },
  {
    "year": 2022,
    "type": "conference",
    "title": "Krathu-500: Post-Comments Thai Corpus",
    "authors": "Pittawat Taveekitworachai and Jonathan H. Chan",
    "venue": "The Research, Invention, and Innovation Congress (RI2C) 2022",
    "url": "https://ieeexplore.ieee.org/abstract/document/9910314",
    "artifacts": [
      { "label": "Research artifacts", "href": "https://github.com/Pittawat2542/krathu-500" }
    ],
    "tags": ["Natural Language Processing Datasets", "Thai", "Sentiment Analysis"],
    "abstract": "The Krathu-500 contains 574 post titles and a post body with all comments on each post on Pantip.com. The corpus contains a total of 63,293 comments in Thai language that is used in real-life situations, with various contexts and types, in a conversational form. The corpus serves as a good way to improve the capabilities of machine learning techniques that deal with the Thai language. A smaller sentiment-labeled corpus of comments is also provided with 6,306 records. The labeled corpus is a human-annotated dataset with three labels for negative, neutral, and positive comments. Three baseline models were developed for labeled corpus using simple LSTM, CNN, and BERT, respectively. The project also consists of an open-source repository that allows anyone who is interested to modify and built on top of the current source code and dataset."
  },
  {
    "year": 2025,
    "type": "preprint",
    "title": "Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting",
    "authors": "Saksorn Ruangtanusak, Pittawat Taveekitworachai, Kunat Pipatanakul",
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2509.00482",
    "artifacts": [{ "label": "APO Tool", "href": "https://github.com/scb-10x/apo" }],
    "tags": ["Large Language Models", "Prompt Engineering"],
    "abstract": "This report investigates approaches for prompting a tool-augmented large language model (LLM) to act as a role-playing dialogue agent in the API track of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this setting, dialogue agents often produce overly long in-character responses (over-speaking) while failing to use tools effectively according to the persona (under-acting), such as generating function calls that do not exist or making unnecessary tool calls before answering. We explore four prompting approaches to address these issues: 1) basic role prompting, 2) human-crafted role prompting, 3) automatic prompt optimization (APO), and 4) rule-based role prompting. The rule-based role prompting (RRP) approach achieved the best performance through two novel techniques--character-card/scene-contract design and strict enforcement of function calling--which led to an overall score of 0.571, improving on the zero-shot baseline score of 0.519. These findings demonstrate that RRP design can substantially improve the effectiveness and reliability of role-playing dialogue agents compared with more elaborate methods such as APO. To support future efforts in developing persona prompts, we are open-sourcing all of our best-performing prompts and the APO tool."
  },
  {
    "year": 2025,
    "type": "preprint",
    "title": "FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning",
    "authors": "Natapong Nitarach, Warit Sirichotedumrong, Panop Pitchayarthorn, Pittawat Taveekitworachai, Potsawee Manakul, and Kunat Pipatanakul",
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2506.16123",
    "artifacts": [],
    "tags": ["Large Language Models", "Prompt Engineering", "Reasoning Models"],
    "abstract": "This paper presents FinCoT, a structured chain-of-thought (CoT) prompting framework that embeds domain-specific expert financial reasoning blueprints to guide large language models' behaviors. We identify three main prompting styles in financial NLP (FinNLP): (1) standard prompting (zero-shot), (2) unstructured CoT (free-form reasoning), and (3) structured CoT (with explicitly structured reasoning steps). Prior work has mainly focused on the first two, while structured CoT remains underexplored and lacks domain expertise incorporation. Therefore, we evaluate all three prompting approaches across ten CFA-style financial domains and introduce FinCoT as the first structured finance-specific prompting approach incorporating blueprints from domain experts. FinCoT improves the accuracy of a general-purpose model, Qwen3-8B-Base, from 63.2% to 80.5%, and boosts Fin-R1 (7B), a finance-specific model, from 65.7% to 75.7%, while reducing output length by up to 8.9x and 1.16x compared to structured CoT methods, respectively. We find that FinCoT proves most effective for models lacking financial post-training. Our findings show that FinCoT does not only improve performance and reduce inference costs but also yields more interpretable and expert-aligned reasoning traces."
  },
  {
    "year": 2024,
    "type": "preprint",
    "title": "Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models",
    "authors": "Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, and Kasima Tharnpipitchai",
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2412.13702",
    "artifacts": [
      {
        "label": "Text Models",
        "href": "https://huggingface.co/collections/scb10x/typhoon-2-text-675e58b48747ba8659895538"
      },
      {
        "label": "Multimodal Models",
        "href": "https://huggingface.co/collections/scb10x/typhoon-2-multimodal-675e59368326ac2328c8210f"
      }
    ],
    "tags": ["Large Language Models", "Large Multimodal Models"],
    "abstract": "This paper introduces Typhoon 2, a series of text and multimodal large language models optimized for the Thai language. The series includes models for text, vision, and audio. Typhoon2-Text builds on state-of-the-art open models, such as Llama 3 and Qwen2, and we perform continual pre-training on a mixture of English and Thai data. We employ post-training techniques to enhance Thai language performance while preserving the base models' original capabilities. We release text models across a range of sizes, from 1 to 70 billion parameters, available in both base and instruction-tuned variants. To guardrail text generation, we release Typhoon2-Safety, a classifier enhanced for Thai cultures and language. Typhoon2-Vision improves Thai document understanding while retaining general visual capabilities, such as image captioning. Typhoon2-Audio introduces an end-to-end speech-to-speech model architecture capable of processing audio, speech, and text inputs and generating both text and speech outputs."
  },
  {
    "year": 2024,
    "type": "preprint",
    "title": "Multiverse of Greatness: Generating Story Branches with LLMs",
    "authors": "Pittawat Taveekitworachai, Chollakorn Nimpattanavong, Mustafa Can Gursesli, Antonio Lanata, Andrea Guazzini, and Ruck Thawonmas",
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2411.14672",
    "artifacts": [
      {
        "label": "Research artifacts",
        "href": "https://github.com/orgs/multiverse-of-greatness/repositories"
      }
    ],
    "tags": ["Large Language Models", "Low-resource LLMs"],
    "abstract": "This paper presents Dynamic Context Prompting/Programming (DCP/P), a novel framework for interacting with LLMs to generate graph-based content with a dynamic context window history. While there is an existing study utilizing LLMs to generate a visual novel game, the previous study involved a manual process of output extraction and did not provide flexibility in generating a longer, coherent story. We evaluate DCP/P against our baseline, which does not provide context history to an LLM and only relies on the initial story data. Through objective evaluation, we show that simply providing the LLM with a summary leads to a subpar story compared to additionally providing the LLM with the proper context of the story. We also provide an extensive qualitative analysis and discussion. We qualitatively examine the quality of the objectively best-performing generated game from each approach. In addition, we examine biases in word choices and word sentiment of the generated content. We find a consistent observation with previous studies that LLMs are biased towards certain words, even with a different LLM family. Finally, we provide a comprehensive discussion on opportunities for future studies."
  }
]
